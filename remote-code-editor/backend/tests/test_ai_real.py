"""
AIåŠŸèƒ½æµ‹è¯•è„šæœ¬
ç›´æ¥æµ‹è¯•æ™ºè°±AI APIè¿æ¥å’Œå“åº”
"""
import asyncio
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv()

from services.ai_service import AIService

async def test_ai_service():
    """æµ‹è¯•AIæœåŠ¡"""
    api_key = os.getenv("ZHIPU_API_KEY")
    
    if not api_key:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ° ZHIPU_API_KEY ç¯å¢ƒå˜é‡")
        return False
    
    print(f"âœ“ æ‰¾åˆ° API Key: {api_key[:20]}...")
    
    # åˆå§‹åŒ–æœåŠ¡
    try:
        service = AIService(api_key)
        print(f"âœ“ AIæœåŠ¡åˆå§‹åŒ–æˆåŠŸï¼Œé»˜è®¤æ¨¡å‹: {service.default_model}")
    except Exception as e:
        print(f"âŒ AIæœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•ç®€å•å¯¹è¯
    print("\n" + "="*50)
    print("æµ‹è¯•1: ç®€å•å¯¹è¯ï¼ˆæµå¼ï¼‰")
    print("="*50)
    
    messages = [{"role": "user", "content": "è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä¸€ä¸‹Pythonè¯­è¨€"}]
    
    try:
        full_response = ""
        async for chunk in service.chat_stream(messages):
            if chunk.get("type") == "content":
                content = chunk.get("content", "")
                full_response += content
                print(content, end="", flush=True)
            elif chunk.get("type") == "error":
                print(f"\nâŒ é”™è¯¯: {chunk.get('message')}")
                return False
        
        print()  # æ¢è¡Œ
        if full_response:
            print(f"âœ“ æµå¼å¯¹è¯æµ‹è¯•æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(full_response)} å­—ç¬¦")
        else:
            print("âš  å“åº”ä¸ºç©º")
            
    except Exception as e:
        print(f"\nâŒ æµå¼å¯¹è¯æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•åŒæ­¥å¯¹è¯
    print("\n" + "="*50)
    print("æµ‹è¯•2: åŒæ­¥å¯¹è¯")
    print("="*50)
    
    messages = [{"role": "user", "content": "1+1ç­‰äºå¤šå°‘ï¼Ÿåªå›ç­”æ•°å­—"}]
    
    try:
        result = await service.chat_sync(messages)
        print(f"å“åº”: {result}")
        if result:
            print(f"âœ“ åŒæ­¥å¯¹è¯æµ‹è¯•æˆåŠŸ")
        else:
            print("âš  å“åº”ä¸ºç©º")
    except Exception as e:
        print(f"âŒ åŒæ­¥å¯¹è¯æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•ä¸åŒæ¨¡å‹
    print("\n" + "="*50)
    print("æµ‹è¯•3: åˆ‡æ¢æ¨¡å‹æµ‹è¯•")
    print("="*50)
    
    models = ["glm-4-flash", "glm-4.7-flash"]
    for model in models:
        try:
            service.set_model(model)
            print(f"\nä½¿ç”¨æ¨¡å‹: {model}")
            messages = [{"role": "user", "content": "ä½ å¥½"}]
            result = await service.chat_sync(messages)
            if result:
                print(f"âœ“ æ¨¡å‹ {model} æµ‹è¯•æˆåŠŸ")
            else:
                print(f"âš  æ¨¡å‹ {model} å“åº”ä¸ºç©º")
        except Exception as e:
            print(f"âŒ æ¨¡å‹ {model} æµ‹è¯•å¤±è´¥: {e}")
    
    print("\n" + "="*50)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
    print("="*50)
    return True

if __name__ == "__main__":
    asyncio.run(test_ai_service())